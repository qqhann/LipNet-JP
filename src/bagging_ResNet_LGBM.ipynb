{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype_float = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A', 'I', 'U', 'E', 'O', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = Path('/home/jphacks/LipNet-JP/')\n",
    "youtube_id = '1'\n",
    "# youtube_id = '2'\n",
    "spk = 's{}'.format(youtube_id)\n",
    "txtpath = workdir / 'data/align' / 'output{}word.align'.format(youtube_id)\n",
    "aligned_lm_path = Path('/home/jphacks/LipNet-JP/data/processed2/{0}/{0}_aligned.csv'.format(youtube_id))\n",
    "lm_path = Path('/home/jphacks/LipNet-JP/data/processed/{0}/{0}.csv'.format(youtube_id))\n",
    "croppeddir = Path('/home/jphacks/LipNet-JP/data/processed2/{0}/{0}_aligned_aligned_cropped'.format(youtube_id))\n",
    "assert croppeddir.exists()\n",
    "\n",
    "datadir = Path('/home/jphacks/LipNet-JP/data')\n",
    "videodir = datadir / 'lip_video'\n",
    "txtdir = datadir / 'align_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inwidth, inheight = 160, 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_lm_df = pd.read_csv(str(aligned_lm_path))\n",
    "aligned_lm_df['timestamp'] = (aligned_lm_df['frame'] - 1) * (1/30)\n",
    "\n",
    "with open(txtpath, 'r') as f:\n",
    "    txt = json.load(f)\n",
    "\n",
    "aligned_lm_df['target'] = -1\n",
    "\n",
    "for word in txt:\n",
    "    for c in word:\n",
    "        aligned_lm_df.loc[(aligned_lm_df.timestamp >= c['start']) & (aligned_lm_df.timestamp < c['end']), 'target']         = letters.index(c['word'].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 話していないときのデータを削除\n",
    "aligned_lm_df = aligned_lm_df[aligned_lm_df['target'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = aligned_lm_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(aligned_lm_df, test_size=.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[(set(train_df.columns) - {'target'})].values\n",
    "train_y = train_df.target.values\n",
    "\n",
    "test_x = test_df[(set(test_df.columns) - {'target'})].values\n",
    "test_y = test_df.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data -> Train Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x, dtype=\"float32\")\n",
    "train_y = np.array(train_y, dtype=\"int32\")\n",
    "\n",
    "test_x = np.array(test_x, dtype=\"float32\")\n",
    "test_y = np.array(test_y, dtype=\"int32\")\n",
    "\n",
    "train_x = torch.from_numpy(train_x)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "test_x = torch.from_numpy(test_x)\n",
    "test_y = torch.from_numpy(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 100\n",
    "HIDDEN_SIZE = 20\n",
    "BATCH_SIZE = 20\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype_float = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['A', 'I', 'U', 'E', 'O', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = Path('/home/jphacks/LipNet-JP/')\n",
    "youtube_id = '1'\n",
    "# youtube_id = '2'\n",
    "spk = 's{}'.format(youtube_id)\n",
    "txtpath = workdir / 'data/align' / 'output{}word.align'.format(youtube_id)\n",
    "aligned_lm_path = Path('/home/jphacks/LipNet-JP/data/processed2/{0}/{0}_aligned.csv'.format(youtube_id))\n",
    "lm_path = Path('/home/jphacks/LipNet-JP/data/processed/{0}/{0}.csv'.format(youtube_id))\n",
    "croppeddir = Path('/home/jphacks/LipNet-JP/data/processed2/{0}/{0}_aligned_aligned_cropped'.format(youtube_id))\n",
    "assert croppeddir.exists()\n",
    "\n",
    "datadir = Path('/home/jphacks/LipNet-JP/data')\n",
    "videodir = datadir / 'lip_video'\n",
    "txtdir = datadir / 'align_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(152),\n",
    "    transforms.CenterCrop(152),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inwidth, inheight = 160, 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_lm_df = pd.read_csv(str(aligned_lm_path))\n",
    "aligned_lm_df['timestamp'] = (aligned_lm_df['frame'] - 1) * (1/30)\n",
    "\n",
    "with open(txtpath, 'r') as f:\n",
    "    txt = json.load(f)\n",
    "\n",
    "aligned_lm_df['target'] = -1\n",
    "\n",
    "for word in txt:\n",
    "#     print(\"\".join([c[\"word\"] for c in word]))\n",
    "    for c in word:\n",
    "        aligned_lm_df.loc[(aligned_lm_df.timestamp >= c['start']) & (aligned_lm_df.timestamp < c['end']), 'target'] = letters.index(c['word'].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "imglist = []\n",
    "targetlist = []\n",
    "_lett_counter = {l: 0 for l in letters}\n",
    "for idx, row in aligned_lm_df.iterrows():\n",
    "    if row.target < 0:\n",
    "        continue\n",
    "    imgpath = croppeddir / 'frame_det_00_{:06d}.bmp'.format(int(row.frame))\n",
    "    img = Image.open(str(imgpath))\n",
    "    input_tensor = preprocess(img)\n",
    "    size = np.asarray(img).shape\n",
    "    if size != (inheight, inwidth, 3):\n",
    "        continue\n",
    "    # img = np.moveaxis(img, 2, 0)  # (80, 160, 3) -> (3, 80, 160)\n",
    "    imglist.append(input_tensor)\n",
    "    targetlist.append(int(row.target))\n",
    "    _lett_counter[letters[int(row.target)]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipDataset(Dataset):\n",
    "    def __init__(self, imglist, targetlist, idxlist):\n",
    "        self.imglist = imglist\n",
    "        self.targetlist = targetlist\n",
    "        self.idxlist = idxlist\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idxlist)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.imglist[self.idxlist[idx]], self.targetlist[self.idxlist[idx]])\n",
    "    \n",
    "    def _to_list(self):\n",
    "        return [self[i] for i in range(len(self))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3196 799 999\n"
     ]
    }
   ],
   "source": [
    "train_idxlist, test_idxlist = train_test_split(list(range(len(imglist))), test_size=.2, shuffle=True)\n",
    "train_idxlist, validate_idxlist = train_test_split(train_idxlist, test_size=.2, shuffle=True)\n",
    "\n",
    "print(len(train_idxlist), len(validate_idxlist), len(test_idxlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lipdataset = LipDataset(imglist, targetlist, train_idxlist)\n",
    "validate_lipdataset = LipDataset(imglist, targetlist, validate_idxlist)\n",
    "test_lipdataset = LipDataset(imglist, targetlist, test_idxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_lipdataset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "validateloader = torch.utils.data.DataLoader(validate_lipdataset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_lipdataset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torchvision.models.resnet34(pretrained=True, num_classes=6).to(device)\n",
    "net = torchvision.models.resnet34(pretrained=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.5, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(iterator):\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        acc = (predicted == target.to(device)).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() / BATCH_SIZE\n",
    "        epoch_acc += acc.item() / BATCH_SIZE\n",
    "        \n",
    "        break\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (data, target) in enumerate(iterator):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "                        \n",
    "            output = model(data.type(dtype_float))\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            acc = (predicted == target.to(device)).sum()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 15s\n",
      "0.00041985158319032747 0.0028089887640449437\n",
      "1.5994130261102517 0.3541927409261577\n",
      "Epoch: 02 | Epoch Time: 0m 15s\n",
      "0.0004141763801431834 0.0024968789013732834\n",
      "1.6137177419006004 0.3591989987484355\n",
      "Epoch: 03 | Epoch Time: 0m 15s\n",
      "0.00039636344647734945 0.0031210986267166045\n",
      "1.621929926030776 0.3541927409261577\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(net, trainloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(net, validateloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(net.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(train_loss, train_acc)\n",
    "    print(valid_loss, valid_acc)\n",
    "#     print(f'\\tTrain Loss: {float(train_loss):.3f} | Train Acc: {float(train_acc*100):.2f}%')\n",
    "#     print(f'\\t Val. Loss: {float(valid_loss):.3f} |  Val. Acc: {float(valid_acc*100):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, optimizer, criterion):\n",
    "    model.eval()\n",
    "    class_correct = [0.] * len(letters)\n",
    "    class_total = [0.] * len(letters)\n",
    "    with torch.no_grad():\n",
    "        for data in iterator:\n",
    "            images, labels = data\n",
    "            outputs = net(images.type(dtype_float))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels.to(device)).squeeze()\n",
    "            class_correct[labels.item()] += c.item() * 1\n",
    "            class_total[labels.item()] += 1\n",
    "\n",
    "    for i, l in enumerate(letters):\n",
    "        print('Accuracy of    {}: {:.4f} ({:4d}/{:4d})'.format(l, class_correct[i]/class_total[i] if class_total[i] > 0 else 0, int(class_correct[i]), int(class_total[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net, testloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_float = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(152),\n",
    "    transforms.CenterCrop(152),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet34(pretrained=True).to(device)\n",
    "model.load_state_dict(torch.load(\"tut5-model.pt\"))\n",
    "\n",
    "img = Image.open(str(list(croppeddir.iterdir())[0]))\n",
    "input_tensor = preprocess(img)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "outputs = model(input_batch.type(dtype_float))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "predict_label = predicted.squeeze().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging using ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(all_df, test_size=.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[(set(train_df.columns) - {'target'})].values\n",
    "train_y = train_df.target.values\n",
    "\n",
    "test_x = test_df[(set(test_df.columns) - {'target'})].values\n",
    "test_y = test_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
    "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
    "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
    "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994998749687422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(train_x), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.658"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(test_x), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of    A: 0.7311 ( 193/ 264)\n",
      "Accuracy of    I: 0.6452 ( 120/ 186)\n",
      "Accuracy of    U: 0.5978 (  55/  92)\n",
      "Accuracy of    E: 0.5338 (  71/ 133)\n",
      "Accuracy of    O: 0.7339 ( 160/ 218)\n",
      "Accuracy of    N: 0.5514 (  59/ 107)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_x)\n",
    "\n",
    "class_correct = [0] * len(letters)\n",
    "class_total = [0] * len(letters)\n",
    "\n",
    "for p, a in zip(pred, test_y):\n",
    "    class_total[a] += 1\n",
    "    if p == a:\n",
    "        class_correct[a] += 1\n",
    "\n",
    "for i, l in enumerate(letters):\n",
    "    print('Accuracy of    {}: {:.4f} ({:4d}/{:4d})'.format(l, class_correct[i]/class_total[i], class_correct[i], class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f18d1b70a90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQmUlEQVR4nO3de6xlZXnH8e/vnBkEAUWlGpxBwQAavFTrODXBEkUriARoYtuhtVpLnKTFW02qEv8gpqWtvXhJTNNOAS/RgIhaiSiKFTVaucpoueoIBGZiHalSJFBgmKd/nA3uTjj7dvbmPWvz/SQr7P2uvdd6IOE37zzrXWunqpAkPfoWWhcgSY9VBrAkNWIAS1IjBrAkNWIAS1IjBrAkNWIAS1Ija4Z9IMlzgJOAdb2hHcCFVXXDLAuTpHk3cAac5N3AeUCAK3pbgHOTvGf25UnS/MqgO+GS/BB4blU9sMf4XsB1VXX4Mt/bDGwGWPOkDS9es99h06t4xnb86JTWJYxtr4UntC5hbLvq3tYljOW+Bx8Y/qFVZt81+7UuYWz7rT0mKz3GPs84ZeTbe++97dwVn28lhvWAdwNPf4Txg3r7HlFVbamqDVW1oUvhK0mPpmE94HcA/57kR8DtvbFnAIcBb5llYZI0iaQ7awsGBnBVXZzkCGAj//8i3JVV9eCsi5OkcS1k6NqCVWNopVW1G7jsUahFklZsbmbAktQ1SdPramMxgCXNGWfAktSELQhJasQAlqRG5moVhCR1iTNgSWrEAJakRoLL0CSpCWfAktTIwkJ3Yq07lUrSSJwBS1ITtiAkqREDWJIaiS0ISWrDGbAkNbKwsNi6hJEZwJLmii2IPj/98amzPsVUveBf1rYuYWy3vOWprUsY267d/9u6hLEs5L9blzC2xy0+uXUJTdiCkKRGDGBJasQWhCQ1Em9FlqQ2/FFOSWrEFoQkNeJFOElqxRaEJDXSnQmwASxpzix0J4ENYEnzpTv5awBLmi9lD1iSGulO/nZpsi5JI1jI6NsQSc5JsjPJtXuMvzXJjUmuS/J3feOnJ9mW5KYkxw47vjNgSfNlui2IjwEfAT7xq8PnFcBJwK9X1X1JntobPxLYBDwXeDrwtSRHVNWDyx3cGbCk+bKY0bchqupbwM/3GP5T4G+r6r7eZ3b2xk8Czquq+6rqFmAbsHHQ8Q1gSfMlGXlLsjnJVX3b5hHOcATwW0kuT/LNJC/pja8Dbu/73Pbe2LJsQUiaL2N0IKpqC7BlzDOsAZ4MvBR4CXB+kmeNeYyHDyRJ82OEi2srtB34XFUVcEWS3cCBwA7g4L7Pre+NLcsWhKT5kjG2yfwb8AqAJEcAewF3ABcCm5I8LsmhwOHAFYMO5AxY0lypxenNK5OcC7wcODDJduAM4BzgnN7StPuBN/Zmw9clOR+4HtgFnDZoBQSsIICTvKmqPjrp9yVpJqbYgaiqU5bZ9fplPn8mcOaox1/JHxXvW25H/5XFj5510QpOIUljGmMVRGsDZ8BJfrDcLuBpy32v/8riXQ98rSauTpLGNfuLcFMzrAXxNOBY4Bd7jAf4j5lUJEkr0Z38HRrAXwT2q6qte+5I8o2ZVCRJK7EKWgujGhjAVXXqgH1/MP1yJGmFRrjFeLVwGZqk+TIvM2BJ6pzu5K8BLGm+1BytgpCkbrEFIUmNdCd/DWBJc2aKz4KYNQNY0nxxBixJjXgRTpIaMYAlqY3qTv4awJLmjBfhJKkRWxCS1Eh3JsAGsKQ5451wktSILQhJaqOcAf/K/msPnvUppurm03a1LmFsX7r91tYljO3V6w5qXcJY9lvTrXoBksfo/GqNASxJbTgDlqRG7AFLUiPdyV8DWNJ88RcxJKkVA1iSGvFn6SWpEVdBSFIjtiAkqREDWJLa8FZkSWrFi3CS1IgtCElqxACWpEa6k79d+vEOSRquFjLyNkySc5LsTHJt39jfJ7kxyQ+SfD7JAX37Tk+yLclNSY4ddnwDWNJ8SUbfhvsYcNweY5cAz6uqFwA/BE5fOm2OBDYBz+1955+SLA46uAEsab4sZvRtiKr6FvDzPca+WlUP/XLDZcD63uuTgPOq6r6qugXYBmwcdHwDWNJcWVgYfUuyOclVfdvmMU/3J8CXe6/XAbf37dveG1uWF+EkzZVx7sOoqi3AlsnOk/cCu4BPTfJ9MIAlzZlH40a4JH8MnAC8sqqqN7wD6P8RzPW9sWUNbUEkeU6SVybZb4/xPRvTktRckpG3CY9/HPAu4MSquqdv14XApiSPS3IocDhwxaBjDQzgJG8DvgC8Fbg2yUl9u/96wPce7qts2fLpwf82kjRF4/SAh0lyLvBd4NlJtic5FfgIsD9wSZKtSf4ZoKquA84HrgcuBk6rqgcHHX9YC+LNwIur6u4khwAXJDmkqj7MgOXO/X2V4qZa7nOSNG2Z4tKCqjrlEYbPHvD5M4EzRz3+sABeqKq7ewe+NcnLWQrhZ9Kp+00kPVZ06GFoQ3vAP03ywofe9ML4BOBA4PmzLEySJrGQ0bfWhgXwG4D/6h+oql1V9Qbg6JlVJUkTmu6NcLM1sAVRVdsH7PvO9MuRpJVZDcE6KtcBS5orCz6QXZLacAYsSY0YwJLUiAEsSY2shuVlozKAJc0VZ8CS1IirICSpEWfAktSIASxJjRjAktSIqyAkqZGFgT8Ev7oYwJLmii0ISWpk0t96a8EAljRXOpS/BrCk+WIA99ldD8z6FFOVDv7U3fEHH9K6hLH9zdZln/W/Kp3+wvWtSxhb1/7fA5jGTWwGsCQ1smaKv4o8awawpLmykGpdwsgMYElzxRsxJKmRDnUgDGBJ88UWhCQ1YgtCkhpZYwBLUhuxBSFJbdiCkKRGXAUhSY24CkKSGvEinCQ1Yg9YkhqxBSFJjXRpBtylC4aSNNTCGNswSf48yXVJrk1ybpK9kxya5PIk25J8OsleK6lVkubGQmrkbZAk64C3ARuq6nnAIrAJeD/wwao6DPgFcOrEtU76RUlajdYsjL6NcjhgnyRrgMcDPwGOAS7o7f84cPKktRrAkubKOC2IJJuTXNW3bX7oOFW1A/gH4DaWgvd/gKuBO6tqV+9j24F1k9Y69CJcko1LtdSVSY4EjgNurKovTXpSSZqVcVZBVNUWYMsj7UvyJOAk4FDgTuAzLOXf1AwM4CRnAK8B1iS5BPhN4FLgPUleVFVnTrMYSVqpKa6CeBVwS1X9DCDJ54CjgAOSrOnNgtcDOyY9wbAWxOt6JzwaOA04uar+EjgW+P3lvtQ/rf/XLZ+ZtDZJGtsUV0HcBrw0yeOTBHglcD1Lk9DX9T7zRuALk9Y6rAWxq6oeBO5J8uOqugugqu5Nsnu5L/VP6x+sa7uzKlpS501rBlxVlye5APgesAu4hqVcuwg4L8lf9cbOnvQcwwL4/iSPr6p7gBc/NJjkicCyASxJrSwuTG/OV1VnAGfsMXwzsHEaxx8WwEdX1X29QvoDdy1LU29JWlW6tLRrYAA/FL6PMH4HcMdMKpKkFfBZEJLUSJeeBWEAS5orBrAkNbLWFoQkteEMWJIaMYAlqZFFA1iS2nAGLEmNuA5YkhpZ6wxYktqwBSFJjdiCkKRGXAUhSY3YgpCkRkb8teNVwQCWNFcW7QFLUhsdmgDPPoB/+cBtsz7FVO2uXa1LGNs+iwe2LmFsf3bkA61LGMsRr/pO6xLGtvXiw1uXMLZ9p5BI9oAlqREDWJIasQcsSY24CkKSGrEFIUmNeCecJDXisyAkqZEOtYANYEnzxR6wJDWydsEWhCQ14QxYkhoxgCWpES/CSVIjcQYsSW3YgpCkRmxBSFIj8U44SWqjQx2ITs3WJWmoZPRttONlMck1Sb7Ye39oksuTbEvy6SR7TVqrASxprmSMbURvB27oe/9+4INVdRjwC+DUSWsdO4CTfGLSk0nSrC1m9G2YJOuB1wJn9d4HOAa4oPeRjwMnT1rrwB5wkgv3HAJekeQAgKo6cdITS9IsTHkd8IeAdwH7994/Bbiz6uFf790OrJv04MNmwOuBu4APAP/Y237Z9/oRJdmc5KokV33srC9PWpskjW2cFkR/VvW2zQ8fJzkB2FlVV8+q1mGrIDaw1P94L/AXVbU1yb1V9c1BX6qqLcAWgDvv/1J31oRI6rxxJsD9WfUIjgJOTHI8sDfwBODDwAFJ1vRmweuBHZPWOnAGXFW7q+qDwJuA9yb5CC5dk7SKLWT0bZCqOr2q1lfVIcAm4OtV9YfApcDreh97I/CFiWsd5UNVtb2qfhf4MvDJSU8mSbM2g1UQe3o38M4k21jqCZ896YHGms1W1UXARZOeTJJmbRa/CVdV3wC+0Xt9M7BxGse1nSBprvg0NElqpEt3lxnAkuaKM2BJaqRD+WsAS5ovPpBdkhoxgCWpkQ7lrwEsab74ixiS1IgzYElqxGVoktTIYusCxmAAS5orzoAlqZnuJLABLGmuxACWpDaS7jyOxwCWNGecAUtSE+nQAykfhQDuzp9GAPuvPbh1CWNbu7Bv6xLGtjdPaV3CWG665JmtSxjbBbfc3rqEsf3es1Z+DFsQktRMdyZ9BrCkueIqCElqxACWpEaS7tyMbABLmjPOgCWpCVsQktSMy9AkqQlnwJLUSDr0PEoDWNJcSYceyW4AS5ozzoAlqQlbEJLUjAEsSU34OEpJasYZsCQ1seDzgCWpFQNYkprwTjhJamZOAzjJy4CNwLVV9dXZlCRJk+vSOuCBzZIkV/S9fjPwEWB/4Iwk75lxbZI0trA48tZaqmr5nck1VfWi3usrgeOr6mdJ9gUuq6rnL/O9zcDm3tstVbVlynU/fJ5ZHXsWulYvdK/mrtUL1vxYNiyAvw+8nKWZ8leqakPfvofDuZUkV/XXtNp1rV7oXs1dqxes+bFsWA/4icDVLHW1K8lBVfWTJPvRpU63JK1CAwO4qg5ZZtdu4HemXo0kPYZMtAytqu4BbplyLZPoWg+qa/VC92ruWr1gzY9ZA3vAkqTZ6c49e5I0ZwxgSWqkkwGc5LgkNyXZ1oUbQpKck2Rnkmtb1zKKJAcnuTTJ9UmuS/L21jUNk2TvJFck+X6v5ve1rmkUSRaTXJPki61rGUWSW5P8Z5KtSa5qXU/Xda4HnGQR+CHw28B24ErglKq6vmlhAyQ5Grgb+ERVPa91PcMkOQg4qKq+l2R/lpYinrzK/xsH2Leq7k6yFvg28PaquqxxaQMleSewAXhCVZ3Qup5hktwKbKiqO1rXMg+6OAPeCGyrqpur6n7gPOCkxjUNVFXfAn7euo5RVdVPqup7vde/BG4A1rWtarBacnfv7dretqpnF0nWA68Fzmpdi9roYgCvA27ve7+dVR4OXZbkEOBFwOVtKxmu99f5rcBO4JKqWu01fwh4F0vr6ruigK8mubr3yAGtQBcDWI+S3h2PnwXeUVV3ta5nmKp6sKpeCKwHNiZZte2eJCcAO6vq6ta1jOllVfUbwGuA03rtNU2oiwG8Azi47/363pimqNdH/Szwqar6XOt6xlFVdwKXAse1rmWAo4ATez3V84BjknyybUnDVdWO3j93Ap9nqSWoCXUxgK8EDk9yaJK9gE3AhY1rmiu9C1pnAzdU1Qda1zOKJL+W5IDe631Yukh7Y9uqlldVp1fV+t7t/puAr1fV6xuXNVCSfXsXZek9EfHVQCdW9qxWnQvgqtoFvAX4CksXh86vquvaVjVYknOB7wLPTrI9yamtaxriKOCPWJqVbe1tx7cuaoiDgEuT/IClP6QvqapOLO3qkKcB3+49JfEK4KKqurhxTZ3WuWVokjQvOjcDlqR5YQBLUiMGsCQ1YgBLUiMGsCQ1YgBLUiMGsCQ18n+1kACw+wSMMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = np.zeros([len(letters), len(letters)])\n",
    "for p, a in zip(pred, test_y):\n",
    "    confusion_matrix[p, a] += 1\n",
    "    \n",
    "sns.heatmap(confusion_matrix, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
